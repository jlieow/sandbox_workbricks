{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61842d4c-2a40-475e-b685-0800365bbe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e61da78583e2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://0b8ee91998c1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Joins</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff77387620>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Joins\")\n",
    "    .master(\"spark://0b8ee91998c1:7077\")\n",
    "    .config(\"spark.cores.max\", 4)\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70ab4bd-cb70-4de7-b97b-cb7313a5b6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bb46c-8a0c-4f9b-bf78-4141d71f58f5",
   "metadata": {},
   "source": [
    "#### Join Big and Small table - SortMerge vs BroadCast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa89f9b-334a-4e68-a136-e22c272bc12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/join_data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4389aa4-73b7-4221-ab1c-6514dcf3b584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"data/input/join_data/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295111c4-987d-459c-b878-91ec2a9867aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Sort Merge Join\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5adcd5-ef1a-4e8f-9497-b4968161437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast Hash Join\n",
    "# If you are joining a smaller and bigger dataset, Broadcast Joins are more optimal\n",
    "# Best to avoid broadcasting data larger than 1 GB\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a1e973-8a25-4b4c-90f7-79c74d18af31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb129b-c3dc-4d22-85f1-4cd2881ea055",
   "metadata": {},
   "source": [
    "#### Join Big and Big table - SortMerge without Buckets\n",
    "\n",
    "Bucketing in Spark is an optimization technique that divides data into a fixed number of \"buckets\" based on the hash value of one or more specified columns. The main goal of bucketing is to improve the efficiency of join and aggregation operations on large datasets by **avoiding data shuffling** and enabling Spark to read only relevant data when performing these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d30c68-f7a9-4704-bcd9-6989cdd4298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"data/input/join_data/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e1278da-6a48-4035-ad0d-50aa8ce49fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"data/input/join_data/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ef56cb-f60f-4361-bc6e-a0ed9b5853e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Data\n",
    "\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c7abf4-9174-49f6-91fa-a57216697bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66442cc2-6645-4a1d-988e-a446bf0ecb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [city_id#87], [city_id#88], LeftOuter\n",
      ":- *(1) Sort [city_id#87 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(city_id#87, 200), ENSURE_REQUIREMENTS, [plan_id=292]\n",
      ":     +- FileScan csv [transacted_at#82,trx_id#83,retailer_id#84,description#85,amount#86,city_id#87] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/data/input/join_data/new_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n",
      "+- *(3) Sort [city_id#88 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(city_id#88, 200), ENSURE_REQUIREMENTS, [plan_id=304]\n",
      "      +- *(2) Filter isnotnull(city_id#88)\n",
      "         +- FileScan csv [city_id#88,city#89,state#90,state_abv#91,country#92] Batched: false, DataFilters: [isnotnull(city_id#88)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/data/input/join_data/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that a Shuffle (Exchange) is performed as part of the join\n",
    "# In a Spark explain plan, \"Exchange\" refers to a physical operation where data is shuffled or redistributed across different partitions or nodes in the cluster,\n",
    "df_sales_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fdded-a234-4b83-b08f-06860ca75522",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Write Sales and City data in Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb3b217-40aa-4ccd-81fd-f4a5ad7be5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write Sales data in Buckets\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"data/output/join_data/sales_bucket.csv\").saveAsTable(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc98941-e3f9-4b3d-8e6b-beac05bc658c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write City data in Buckets\n",
    "\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"data/output/join_data/city_bucket.csv\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59887b0-f929-414a-9237-6d8a4d7f4baa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check tables\n",
    "\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929dc89-fdea-400b-affd-7003faae063d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join Sales and City data - SortMerge with Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f8c248-8bf9-4baf-b0c6-4dc56eb6a16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Sales table\n",
    "\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf9264a-ee16-4f3f-be3c-ca5caa591217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read City table\n",
    "\n",
    "city_bucket = spark.read.table(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cad6d91-c2cc-4873-977e-42c5fa1f1f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "\n",
    "df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19acc427-c43d-4006-b93a-093e0046d074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write dataset\n",
    "# Notice that Shuffle is avoided\n",
    "\n",
    "df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa28032f-b450-459c-9921-b3233713e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) SortMergeJoin [city_id#63], [city_id#64], LeftOuter\n",
      ":- *(1) Sort [city_id#63 ASC NULLS FIRST], false, 0\n",
      ":  +- FileScan csv spark_catalog.default.sales_bucket[transacted_at#58,trx_id#59,retailer_id#60,description#61,amount#62,city_id#63] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/spark-warehouse/data/output/join_data/sales_bucket.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n",
      "+- *(2) Sort [city_id#64 ASC NULLS FIRST], false, 0\n",
      "   +- *(2) Filter isnotnull(city_id#64)\n",
      "      +- FileScan csv spark_catalog.default.city_bucket[city_id#64,city#65,state#66,state_abv#67,country#68] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#64)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/spark-warehouse/data/output/join_data/city_bucket.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that Shuffle is avoided\n",
    "df_joined_bucket.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ba247-8006-481d-98c1-537359cfe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View how tasks are reading Bucket data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec46385-a66d-4fb3-bbb1-a42d0eefd65e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5c608d0-8fcb-4b61-b146-5fc0e1a4f4ad",
   "metadata": {},
   "source": [
    "Scenarios Which Results in Shuffles Despite Using Buckets\n",
    "\n",
    "1. Joining Column different than Bucket Column, Same Bucket Size - Shuffle on Both table\n",
    "2. Joining Column Same, One table in Bucket - Shuffle on non Bucket table\n",
    "3. Joining Column Same, Different Bucket Size - Shuffle on Smaller Bucket Side\n",
    "4. Joining Column Same, Same Bucket Size - No Shuffle (Faster Join)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93b1676c-c00d-4a0e-a5cb-7fbf16f110c9",
   "metadata": {},
   "source": [
    "1. Decide on the right number of Buckets, too many buckets with insufficient data can lead to small file issue. This issue refers to performance problems that arise when processing many tiny files rather than fewer, larger ones. \n",
    "2. If datasets are Small Shuffle Hash Join may be more appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786056d1-30c6-41cb-938f-52c1595be1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
